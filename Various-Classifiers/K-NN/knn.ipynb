{
 "metadata": {
  "name": "",
  "signature": "sha256:d0dc42742a5558af04fc34eed57d025708c190d5ce9da1f3d8e0b9b51411da1f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import io, spatial, misc\n",
      "from matplotlib import pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q1.1 K-Means Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RAW_MNIST_DATA = io.loadmat('mnist_data/images.mat')\n",
      "MNIST_DATA_IMAGES = np.reshape(RAW_MNIST_DATA['images'], (784, 60000)).transpose().astype(float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class KMeans(object):\n",
      "    def __init__(self, k=10, iters=300, threshold=0.0001):\n",
      "        self.num_clusters = k\n",
      "        self.centroids = {}\n",
      "        self.max_iters = iters\n",
      "        self.threshold = threshold\n",
      "        \n",
      "        for i in range(self.num_clusters):\n",
      "            self.centroids[i] = np.random.choice(256, 784)\n",
      "            \n",
      "    def train(self, data):\n",
      "        iters = 0\n",
      "        prev_J = 0.0\n",
      "        \n",
      "        while iters < self.max_iters:\n",
      "            group = {}\n",
      "            J = 0.0\n",
      "            index = 0\n",
      "            \n",
      "            for s in data:\n",
      "                lowest_dist = 99999.0\n",
      "                lowest_c = -1\n",
      "                \n",
      "                for c in self.centroids:\n",
      "                    centroid = self.centroids[c]\n",
      "                    dist = spatial.distance.euclidean(s, centroid)\n",
      "                    \n",
      "                    if dist < lowest_dist: \n",
      "                        lowest_dist = dist\n",
      "                        lowest_c = c\n",
      "                        \n",
      "                if lowest_c not in group:\n",
      "                    group[lowest_c] = []\n",
      "                \n",
      "                group[lowest_c].append(index)\n",
      "                \n",
      "                index += 1\n",
      "                        \n",
      "            for c in group:\n",
      "                samples = data[group[c]]\n",
      "                new_centroid = np.mean(samples, axis=0)\n",
      "                self.centroids[c] = new_centroid\n",
      "                \n",
      "                for s in samples:\n",
      "                    J += spatial.distance.euclidean(s, new_centroid)\n",
      "                    \n",
      "            if np.absolute(J - prev_J) < self.threshold:\n",
      "                print J\n",
      "                return\n",
      "            else:\n",
      "                prev_J = J\n",
      "                \n",
      "            iters += 1\n",
      "\n",
      "        print \"Loss: %f\" % prev_J"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "km5 = KMeans(5, 30, 1000)\n",
      "km5.train(MNIST_DATA_IMAGES)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "km10 = KMeans(10, 30, 1000)\n",
      "km10.train(MNIST_DATA_IMAGES)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "km20 = KMeans(20, 30, 1000)\n",
      "km20.train(MNIST_DATA_IMAGES)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q1.2 Visualizing Centroids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n in km5.centroids:\n",
      "    misc.imsave('q1_imgs/km5/' + str(n) + '.jpg', km5.centroids[n].reshape(28,28))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n in km10.centroids:\n",
      "    misc.imsave('q1_imgs/km10/' + str(n) + '.jpg', km10.centroids[n].reshape(28,28))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n in km20.centroids:\n",
      "    misc.imsave('q1_imgs/km20/' + str(n) + '.jpg', km20.centroids[n].reshape(28,28))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q1.3 Re-initialization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "km10_2 = KMeans(10, 30, 1000)\n",
      "km10_2.train(MNIST_DATA_IMAGES)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q2.1 Joke Recommender"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RAW_JOKE_DATA = io.loadmat('joke_data/joke_train.mat')\n",
      "JOKE_TRAIN_DATA = RAW_JOKE_DATA['train']\n",
      "JOKE_VALIDATE_LOC = 'joke_data/validation.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q2.2 Warm Up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class JokeAverage(object):\n",
      "    def __init__(self):\n",
      "        self.averages = None\n",
      "        \n",
      "    def train(self, data):\n",
      "        self.averages = np.nanmean(data, axis=0)\n",
      "        \n",
      "    def predict(self, file_loc):\n",
      "        preds = []\n",
      "        \n",
      "        with open(file_loc) as f:\n",
      "            for line in f:\n",
      "                joke_idx = line.split(',')[1]\n",
      "                avg_rating = self.averages[int(joke_idx) - 1]\n",
      "\n",
      "                if avg_rating > 0:\n",
      "                    preds.append(1)\n",
      "                else:\n",
      "                    preds.append(0)\n",
      "                \n",
      "        return preds\n",
      "\n",
      "    \n",
      "class JokeAdvance(object):\n",
      "    def __init__(self, k=10):\n",
      "        self.k = k\n",
      "\n",
      "    def train(self, data):\n",
      "        self.data = np.nan_to_num(data)\n",
      "\n",
      "    def predict(self, file_loc):\n",
      "        preds = []\n",
      "        users = {}\n",
      "        \n",
      "        with open(file_loc) as f:\n",
      "            for line in f:\n",
      "                user_idx = int(line.split(',')[0]) - 1\n",
      "                joke_idx = int(line.split(',')[1]) - 1\n",
      "                \n",
      "                if user_idx not in users:\n",
      "                    closest = np.empty((self.k, 2))\n",
      "                    closest[:] = 999999\n",
      "                    tar_user_ratings = self.data[user_idx]\n",
      "\n",
      "                    for i in range(self.data.shape[0]):\n",
      "                        if i != user_idx:\n",
      "                            dist = spatial.distance.euclidean(tar_user_ratings, self.data[i])\n",
      "\n",
      "                            if np.amax(closest, axis=0)[0] > dist:\n",
      "                                min_idx = np.argmax(closest, axis=0)[0]\n",
      "                                closest[min_idx] = np.asarray([dist, i])\n",
      "                            \n",
      "                    users[user_idx] = closest[:, 1]\n",
      "\n",
      "                closest_users = self.data[users[user_idx].astype(int)]\n",
      "                averages = np.mean(closest_users, axis=0)\n",
      "                avg_rating = averages[joke_idx]\n",
      "\n",
      "                if avg_rating > 0:\n",
      "                    preds.append(1)\n",
      "                else:\n",
      "                    preds.append(0)\n",
      "\n",
      "        return preds\n",
      "    \n",
      "    \n",
      "def accuracy(pred, file_loc):\n",
      "    hits = 0.0\n",
      "    count = 0\n",
      "    \n",
      "    with open(file_loc) as f:\n",
      "        for line in f:\n",
      "            joke_rate = int(line.split(',')[2])\n",
      "\n",
      "            if pred[count] == joke_rate:\n",
      "                hits += 1.0\n",
      "\n",
      "            count += 1\n",
      "            \n",
      "    print \"Accuracy: %f\" % (hits/count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Average Joke Predictor Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joke_avg_clf = JokeAverage()\n",
      "joke_avg_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_avg_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Advance Joke Predictor Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joke_adv_clf = JokeAdvance(10)\n",
      "joke_adv_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_adv_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joke_adv_clf = JokeAdvance(100)\n",
      "joke_adv_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_adv_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joke_adv_clf = JokeAdvance(1000)\n",
      "joke_adv_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_adv_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q2.3 Latent Factor Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def PCA(data, d=10):\n",
      "    k = np.minimum(*data.shape)\n",
      "    \n",
      "    if d > k:\n",
      "        return (None, None)\n",
      "    \n",
      "    data = np.nan_to_num(data)\n",
      "    data -= np.mean(data, axis=0)\n",
      "    U, S, V_T = np.linalg.svd(data, full_matrices=False)\n",
      "    \n",
      "    SORT_IDX = np.argsort(S)[::-1]\n",
      "    U = U[:, SORT_IDX]\n",
      "    S = np.diag(S[SORT_IDX])\n",
      "    V = V_T.T[:, SORT_IDX]\n",
      "    \n",
      "    u = np.dot(U[:, :d], np.sqrt(S[:d, :d]))\n",
      "    v = np.dot(np.sqrt(S[:d, :d]), V[:, :d].T)\n",
      "    \n",
      "    return (u, v)\n",
      "\n",
      "\n",
      "def MSE(data, d=10):\n",
      "    u, v = PCA(data, d)\n",
      "    \n",
      "    if u is None or v is None:\n",
      "        print \"Bad d!\"\n",
      "        return\n",
      "    \n",
      "    MSE = np.nansum((np.dot(u, v) - data)**2)\n",
      "    \n",
      "    return MSE\n",
      "\n",
      "\n",
      "def MSEBetter(data, u, v, c=0.1):\n",
      "    MSE = np.nansum((np.dot(u, v) - data)**2) + c * (np.sum(np.multiply(u, u)) + np.sum(np.multiply(v, v)))\n",
      "    \n",
      "    MSE_du = 2 * (np.dot((np.nan_to_num(np.dot(u, v) - JOKE_TRAIN_DATA)), v.T) + c * u)\n",
      "    MSE_dv = 2 * (np.dot(u.T, np.nan_to_num((np.dot(u, v) - JOKE_TRAIN_DATA))) + c * v)\n",
      "    \n",
      "    return (MSE, MSE_du, MSE_dv)\n",
      "\n",
      "\n",
      "class JokePredictor(object):\n",
      "    def __init__(self, k=10):\n",
      "        self.k = k\n",
      "        self.u = None\n",
      "        self.v = None\n",
      "\n",
      "    def train(self, data):\n",
      "        self.u, self.v = PCA(data, self.k)\n",
      "        \n",
      "        if self.u is None or self.v is None:\n",
      "            print \"Bad d!\"\n",
      "            return\n",
      "        \n",
      "    def predict(self, file_loc):\n",
      "        preds = []\n",
      "        users = {}\n",
      "        \n",
      "        with open(file_loc) as f:\n",
      "            for line in f:\n",
      "                user_idx = int(line.split(',')[0]) - 1\n",
      "                joke_idx = int(line.split(',')[1]) - 1\n",
      "                \n",
      "                pred_rating = np.dot(self.u[user_idx, :], self.v[:, joke_idx])\n",
      "                \n",
      "                if pred_rating > 0:\n",
      "                    preds.append(1)\n",
      "                else:\n",
      "                    preds.append(0)\n",
      "\n",
      "        return preds\n",
      "    \n",
      "    \n",
      "class JokePredictorBetter(object):\n",
      "    def __init__(self, k=10):\n",
      "        self.k = k\n",
      "        self.u = None\n",
      "        self.v = None\n",
      "\n",
      "    def train(self, data, rate=0.1, c=0.1, threshold=100, method='gradient_descent'):\n",
      "        self.u, self.v = PCA(data, self.k)\n",
      "        \n",
      "        if self.u is None or self.v is None:\n",
      "            print \"Bad d!\"\n",
      "            return\n",
      "        \n",
      "        prev_MSE = 0.0\n",
      "        u_or_v = 0\n",
      "        iters = 0\n",
      "        \n",
      "        if method == 'gradient_descent':\n",
      "            while True:\n",
      "                MSE, MSE_du, MSE_dv = MSEBetter(data, self.u, self.v, c)\n",
      "\n",
      "                if np.fabs(prev_MSE - MSE) > threshold:\n",
      "                    self.u -= rate * MSE_du\n",
      "                    self.v -= rate * MSE_dv\n",
      "                    prev_MSE = MSE\n",
      "                    iters += 1\n",
      "\n",
      "                else:\n",
      "                    print \"Converged! Iterations:%d, Loss: %f\" % (iters, MSE)\n",
      "                    return\n",
      "\n",
      "                if iters % 100 == 0:\n",
      "                    print \"Iteration #%d, Loss: %f\" % (iters, MSE)\n",
      "                    \n",
      "        elif method == 'als':\n",
      "            while True:\n",
      "                MSE, MSE_du, MSE_dv = MSEBetter(data, self.u, self.v, c)\n",
      "\n",
      "                if np.fabs(prev_MSE - MSE) > threshold:\n",
      "                    if u_or_v == 0:\n",
      "                        self.u -= rate * MSE_du\n",
      "                        prev_MSE = MSE\n",
      "                        iters += 1\n",
      "                        u_or_v = 1\n",
      "                    \n",
      "                    elif u_or_v == 1:\n",
      "                        self.v -= rate * MSE_dv\n",
      "                        prev_MSE = MSE\n",
      "                        iters += 1\n",
      "                        u_or_v = 0\n",
      "\n",
      "                else:\n",
      "                    print \"Converged! Iterations:%d, Loss: %f\" % (iters, MSE)\n",
      "                    return\n",
      "\n",
      "                if iters % 100 == 0:\n",
      "                    print \"Iteration #%d, Loss: %f\" % (iters, MSE)\n",
      "                    \n",
      "        else:\n",
      "            print \"Unrecognized method!\"\n",
      "            return\n",
      "        \n",
      "        \n",
      "    def predict(self, file_loc):\n",
      "        preds = []\n",
      "        \n",
      "        with open(file_loc) as f:\n",
      "            for line in f:\n",
      "                id\n",
      "                user_idx = int(line.split(',')[0]) - 1\n",
      "                joke_idx = int(line.split(',')[1]) - 1\n",
      "                \n",
      "                pred_rating = np.dot(self.u[user_idx, :], self.v[:, joke_idx])\n",
      "                \n",
      "                if pred_rating > 0:\n",
      "                    preds.append(1)\n",
      "                else:\n",
      "                    preds.append(0)\n",
      "\n",
      "        return preds\n",
      "    \n",
      "    \n",
      "    def predict_kaggle(self, file_loc):\n",
      "        preds =[]\n",
      "        \n",
      "        with open(file_loc) as f:\n",
      "            for line in f:\n",
      "                idx = int(line.split(',')[0])\n",
      "                user_idx = int(line.split(',')[1]) - 1\n",
      "                joke_idx = int(line.split(',')[2]) - 1\n",
      "                \n",
      "                pred_rating = np.dot(self.u[user_idx, :], self.v[:, joke_idx])\n",
      "                \n",
      "                if pred_rating > 0:\n",
      "                    preds.append([idx, 1])\n",
      "                else:\n",
      "                    preds.append([idx, 0])\n",
      "\n",
      "        return preds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q2.3.2 MSE Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d=2\n",
      "mse = MSE(JOKE_TRAIN_DATA, d)\n",
      "print \"MSE where d=%d: %f\" % (d, mse)\n",
      "\n",
      "joke_mse_clf = JokePredictor(d)\n",
      "joke_mse_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_mse_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d=5\n",
      "mse = MSE(JOKE_TRAIN_DATA, d)\n",
      "print \"MSE where d=%d: %f\" % (d, mse)\n",
      "\n",
      "joke_mse_clf = JokePredictor(d)\n",
      "joke_mse_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_mse_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d=10\n",
      "mse = MSE(JOKE_TRAIN_DATA, d)\n",
      "print \"MSE where d=%d: %f\" % (d, mse)\n",
      "\n",
      "joke_mse_clf = JokePredictor(d)\n",
      "joke_mse_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_mse_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d=20\n",
      "mse = MSE(JOKE_TRAIN_DATA, d)\n",
      "print \"MSE where d=%d: %f\" % (d, mse)\n",
      "\n",
      "joke_mse_clf = JokePredictor(d)\n",
      "joke_mse_clf.train(JOKE_TRAIN_DATA)\n",
      "preds = joke_mse_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Q2.3.3 Better Joke Predictor Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d=20\n",
      "joke_better_clf = JokePredictorBetter(d)\n",
      "joke_better_clf.train(JOKE_TRAIN_DATA, 0.0001, 100, 0.1, 'gradient_descent')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preds = joke_better_clf.predict(JOKE_VALIDATE_LOC)\n",
      "accuracy(preds, JOKE_VALIDATE_LOC)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kaggle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "\n",
      "JOKE_KAGGLE_LOC = 'joke_data/query.txt'\n",
      "\n",
      "def save_csv(filename, labels):\n",
      "    with open(filename, 'wb') as csvfile:\n",
      "        writer = csv.writer(csvfile, delimiter=\",\")\n",
      "        writer.writerow([\"Id\", \"Category\"])\n",
      "        count = 1\n",
      "        \n",
      "        for pred in labels:\n",
      "            writer.writerow(pred)\n",
      "            count += 1\n",
      "            \n",
      "def kaggle(filename, data, model):\n",
      "    pred = model.predict_kaggle(data)\n",
      "    save_csv(filename, pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kaggle_clf_gd = JokePredictorBetter(20)\n",
      "kaggle_clf_gd.train(JOKE_TRAIN_DATA, 0.0001, 100, 0.1, 'gradient_descent')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration #100, Loss: 10916342.529750\n",
        "Iteration #200, Loss: 10870274.498576"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #300, Loss: 10857279.348575"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #400, Loss: 10851160.540581"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #500, Loss: 10847709.261574"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #600, Loss: 10845536.568853"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #700, Loss: 10844059.055493"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #800, Loss: 10842995.218540"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #900, Loss: 10842195.557506"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1000, Loss: 10841574.498749"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1100, Loss: 10841079.880672"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1200, Loss: 10840678.170010"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1300, Loss: 10840346.820302"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1400, Loss: 10840070.084904"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1500, Loss: 10839836.603278"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1600, Loss: 10839637.948632"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1700, Loss: 10839467.721242"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1800, Loss: 10839320.963150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #1900, Loss: 10839193.768205"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2000, Loss: 10839083.014396"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2100, Loss: 10838986.175157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2200, Loss: 10838901.183284"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2300, Loss: 10838826.330946"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2400, Loss: 10838760.195096"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2500, Loss: 10838701.581109"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2600, Loss: 10838649.479664"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2700, Loss: 10838603.033370"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2800, Loss: 10838561.510593"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #2900, Loss: 10838524.284666"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3000, Loss: 10838490.817158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3100, Loss: 10838460.644249"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3200, Loss: 10838433.365504"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3300, Loss: 10838408.634532"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3400, Loss: 10838386.151172"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3500, Loss: 10838365.654889"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3600, Loss: 10838346.919191"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3700, Loss: 10838329.746874"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3800, Loss: 10838313.965970"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #3900, Loss: 10838299.426286"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #4000, Loss: 10838285.996437"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #4100, Loss: 10838273.561307"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #4200, Loss: 10838262.019869"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration #4300, Loss: 10838251.283310"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Converged! Iterations:4351, Loss: 10838245.992041"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kaggle('kaggle_submission.txt', JOKE_KAGGLE_LOC, kaggle_clf_gd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kaggle_clf_als = JokePredictorBetter(20)\n",
      "kaggle_clf_als.train(JOKE_TRAIN_DATA, 0.0001, 100, 0.1, 'als')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kaggle('kaggle_submission.txt', JOKE_KAGGLE_LOC, kaggle_clf_als)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}